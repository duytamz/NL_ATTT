import sys
import os
import numpy as np
import csv
import json


# ThÃªm Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a package 'ember2018'
sys.path.insert(0, os.path.join('Dataset_ember_2018'))
from Dataset_ember_2018.ember2018.features import PEFeatureExtractor

# --- Cáº¥u hÃ¬nh ---
file_to_analyze = r"C:\Windows\System32\notepad.exe"
output_csv_path = "notepad_features_detailed.csv"
output_json_path = "notepad_features_summary.json"

print("="*80)
print("EMBER 2018 FEATURE EXTRACTION - VERSION 2 (2381 CHIá»€U)")
print("="*80)
print("Khá»Ÿi táº¡o trÃ¬nh trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng EMBER 2018 Version 2...")
extractor = PEFeatureExtractor(feature_version=2)

print(f"\nBáº¯t Ä‘áº§u phÃ¢n tÃ­ch file: {file_to_analyze}")
try:
    with open(file_to_analyze, "rb") as f:
        file_bytes = f.read()

    # Láº¥y raw features (dictionary format nhÆ° trong EMBER dataset)
    print("\nâ³ Äang trÃ­ch xuáº¥t raw features...")
    raw_features = extractor.raw_features(file_bytes)
    
    # Láº¥y vector Ä‘áº·c trÆ°ng cuá»‘i cÃ¹ng (2381 chiá»u cho version 2)
    print("â³ Äang vector hÃ³a features...")
    feature_vector = extractor.feature_vector(file_bytes)

    print("\n" + "="*80)
    print("âœ… PHÃ‚N TÃCH THÃ€NH CÃ”NG")
    print("="*80)
    print(f"ğŸ“¦ KÃ­ch thÆ°á»›c file:        {len(file_bytes):,} bytes ({len(file_bytes)/1024:.2f} KB)")
    print(f"ğŸ“Š Feature Version:        2 (EMBER 2018)")
    print(f"ğŸ“ Tá»•ng sá»‘ Ä‘áº·c trÆ°ng:      {feature_vector.shape[0]} chiá»u")
    print(f"ğŸ”¢ Kiá»ƒu dá»¯ liá»‡u:           {feature_vector.dtype}")
    
    # --- LÆ¯U RAW FEATURES (JSON) ---
    print(f"\nâ³ Äang lÆ°u raw features vÃ o: {output_json_path}")
    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:
        json.dump(raw_features, jsonfile, indent=2, default=str)
    print("   âœ“ ÄÃ£ lÆ°u raw features (JSON format)!")
    
    # --- LÆ¯U FEATURE VECTOR CHI TIáº¾T (CSV) ---
    print(f"\nâ³ Äang lÆ°u feature vector chi tiáº¿t vÃ o: {output_csv_path}")
    
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # Header cho CSV
        writer.writerow(['FeatureGroup', 'FeatureIndex', 'LocalIndex', 'FeatureName', 'Value'])
        
        current_pos = 0
        
        # Äá»‹nh nghÄ©a cÃ¡c nhÃ³m Ä‘áº·c trÆ°ng theo EMBER 2018 Version 2
        # Tá»•ng: 2381 chiá»u
        feature_groups = [
            ('ByteHistogram', 256),          # Táº§n suáº¥t byte 0x00-0xFF
            ('ByteEntropyHistogram', 256),   # Entropy histogram
            ('StringExtractor', 104),        # String features (paths, URLs, etc.)
            ('GeneralFileInfo', 10),         # ThÃ´ng tin file tá»•ng quÃ¡t
            ('HeaderFileInfo', 62),          # PE Header information
            ('SectionInfo', 255),            # Section table features
            ('ImportsInfo', 1280),           # Import functions (1280 chiá»u)
            ('ExportsInfo', 128),
            ('Datadirectories', 30)             # Export functions
        ]
        
        print("\n   Äang ghi tá»«ng nhÃ³m Ä‘áº·c trÆ°ng...")
        for group_name, expected_dim in feature_groups:
            # Láº¥y slice tÆ°Æ¡ng á»©ng vá»›i nhÃ³m nÃ y
            feature_slice = feature_vector[current_pos : current_pos + expected_dim]
            
            # Ghi tá»«ng giÃ¡ trá»‹
            for local_idx, value in enumerate(feature_slice):
                global_idx = current_pos + local_idx
                
                # Táº¡o tÃªn Ä‘áº·c trÆ°ng cÃ³ Ã½ nghÄ©a
                feature_name = f"{group_name}_{local_idx}"
                
                # ThÃªm tÃªn cá»¥ thá»ƒ hÆ¡n cho má»™t sá»‘ nhÃ³m quan trá»ng
                if group_name == 'ByteHistogram':
                    feature_name = f"Byte_{local_idx:02X}_Count"
                elif group_name == 'ByteEntropyHistogram':
                    feature_name = f"ByteEntropy_Bin_{local_idx}"
                elif group_name == 'GeneralFileInfo':
                    info_names = ['Size', 'VirtualSize', 'HasDebug', 'Exports', 
                                  'Imports', 'HasSignature', 'HasTLS', 'HasResources',
                                  'NumSections', 'Timestamp']
                    if local_idx < len(info_names):
                        feature_name = f"General_{info_names[local_idx]}"
                elif group_name == 'HeaderFileInfo':
                    # CÃ³ thá»ƒ thÃªm tÃªn cá»¥ thá»ƒ cho header fields
                    feature_name = f"Header_{local_idx}"
                elif group_name == 'SectionInfo':
                    # Section features
                    feature_name = f"Section_{local_idx}"
                elif group_name == 'ImportsInfo':
                    # Import function features
                    feature_name = f"Import_{local_idx}"
                elif group_name == 'ExportsInfo':
                    # Export function features
                    feature_name = f"Export_{local_idx}"
                
                row = [group_name, global_idx, local_idx, feature_name, value]
                writer.writerow(row)
            
            current_pos += expected_dim
            print(f"      âœ“ {group_name:30s}: {expected_dim:4d} features")
        
    print("\n   âœ“ ÄÃ£ lÆ°u feature vector thÃ nh cÃ´ng!")
    
    # --- THá»NG KÃŠ Tá»”NG QUAN CHI TIáº¾T ---
    print("\n" + "="*80)
    print("ğŸ“Š THá»NG KÃŠ Äáº¶C TRÆ¯NG CHI TIáº¾T")
    print("="*80)
    print(f"{'NhÃ³m':<30s} {'Sá»‘ chiá»u':>10s} {'Non-zero':>10s} {'Sparsity':>10s}")
    print("-"*80)
    
    current_pos = 0
    total_nonzero = 0
    total_dims = 0
    
    for group_name, dim in feature_groups:
        feature_slice = feature_vector[current_pos : current_pos + dim]
        non_zero = np.count_nonzero(feature_slice)
        sparsity = (dim - non_zero) / dim * 100
        
        total_nonzero += non_zero
        total_dims += dim
        
        print(f"{group_name:<30s} {dim:>10d} {non_zero:>10d} {sparsity:>9.1f}%")
        current_pos += dim
    
    print("-"*80)
    print(f"{'Tá»”NG Cá»˜NG':<30s} {total_dims:>10d} {total_nonzero:>10d} "
          f"{(total_dims-total_nonzero)/total_dims*100:>9.1f}%")
    
    # Thá»‘ng kÃª giÃ¡ trá»‹
    print("\n" + "="*80)
    print("ğŸ“ˆ THá»NG KÃŠ GIÃ TRá»Š")
    print("="*80)
    print(f"Min value:     {feature_vector.min():.6f}")
    print(f"Max value:     {feature_vector.max():.6f}")
    print(f"Mean value:    {feature_vector.mean():.6f}")
    print(f"Median value:  {np.median(feature_vector):.6f}")
    print(f"Std value:     {feature_vector.std():.6f}")
    
    # Kiá»ƒm tra tá»•ng sá»‘ chiá»u
    print("\n" + "="*80)
    print("âœ… XÃC MINH FEATURE VERSION")
    print("="*80)
    if current_pos == 2381:
        print(f"âœ“ ÄÃšNG: Feature Version 2 â†’ {current_pos} chiá»u")
    elif current_pos == 2351:
        print(f"âš ï¸  PhÃ¡t hiá»‡n: Feature Version 1 â†’ {current_pos} chiá»u")
    else:
        print(f"âš ï¸  Cáº£nh bÃ¡o: Sá»‘ chiá»u khÃ´ng khá»›p â†’ {current_pos} chiá»u")
    
    print("\n" + "="*80)
    print("ğŸ“ Káº¾T QUáº¢ ÄÃƒ LÆ¯U")
    print("="*80)
    print(f"1. CSV (chi tiáº¿t):  {output_csv_path}")
    print(f"   â””â”€ {current_pos:,} dÃ²ng (1 header + {current_pos} features)")
    print(f"\n2. JSON (raw):      {output_json_path}")
    print(f"   â””â”€ Dictionary format gá»‘c cá»§a EMBER")
    
    # Hiá»ƒn thá»‹ má»™t vÃ i features máº«u
    print("\n" + "="*80)
    print("ğŸ” MáºªU FEATURES (5 Ä‘áº§u tiÃªn)")
    print("="*80)
    for i in range(min(5, len(feature_vector))):
        print(f"Feature {i:4d}: {feature_vector[i]:.6f}")
    
    print("\n" + "="*80)
    print("âœ… HOÃ€N THÃ€NH!")
    print("="*80)

except FileNotFoundError:
    print(f"\nâŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file táº¡i '{file_to_analyze}'")
    print("\nğŸ’¡ Kiá»ƒm tra:")
    print("   1. ÄÆ°á»ng dáº«n file cÃ³ Ä‘Ãºng khÃ´ng?")
    print("   2. File cÃ³ tá»“n táº¡i khÃ´ng?")
    print("   3. CÃ³ quyá»n Ä‘á»c file khÃ´ng?")
    
except ImportError as e:
    print(f"\nâŒ Lá»–I IMPORT: {e}")
    print("\nğŸ’¡ Kiá»ƒm tra:")
    print("   1. ÄÃ£ cÃ i Ä‘áº·t package ember Ä‘Ãºng chÆ°a?")
    print("   2. ThÆ° má»¥c Dataset_ember_2018 cÃ³ Ä‘Ãºng khÃ´ng?")
    print("   3. File features.py cÃ³ tá»“n táº¡i trong ember2018/ khÃ´ng?")
    
except Exception as e:
    print(f"\nâŒ Lá»–I: {e}")
    print("\nğŸ“‹ Chi tiáº¿t lá»—i:")
    import traceback
    traceback.print_exc()
    
    print("\nğŸ’¡ Gá»£i Ã½:")
    print("   1. Kiá»ƒm tra file PE cÃ³ há»£p lá»‡ khÃ´ng")
    print("   2. File cÃ³ bá»‹ corrupt khÃ´ng")
    print("   3. Äá»§ RAM Ä‘á»ƒ xá»­ lÃ½ file khÃ´ng")

# ThÃªm Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c chá»©a package 'ember2018'
sys.path.insert(0, os.path.join('Dataset_ember_2018'))
from Dataset_ember_2018.ember2018.features import PEFeatureExtractor

# --- Cáº¥u hÃ¬nh ---
file_to_analyze = r"C:\Windows\System32\notepad.exe"
output_csv_path = "notepad_features_detailed.csv"
output_json_path = "notepad_features_summary.json"

print("Khá»Ÿi táº¡o trÃ¬nh trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng EMBER 2018...")
extractor = PEFeatureExtractor(feature_version=2)

print(f"Báº¯t Ä‘áº§u phÃ¢n tÃ­ch file: {file_to_analyze}")
try:
    with open(file_to_analyze, "rb") as f:
        file_bytes = f.read()

    # Láº¥y raw features (dictionary format nhÆ° trong EMBER dataset)
    raw_features = extractor.raw_features(file_bytes)
    
    # Láº¥y vector Ä‘áº·c trÆ°ng cuá»‘i cÃ¹ng (2381 chiá»u cho version 2)
    feature_vector = extractor.feature_vector(file_bytes)

    print("\n--- PHÃ‚N TÃCH THÃ€NH CÃ”NG ---")
    print(f"KÃ­ch thÆ°á»›c file: {len(file_bytes)} bytes")
    print(f"Tá»•ng sá»‘ Ä‘áº·c trÆ°ng: {feature_vector.shape[0]} chiá»u")
    
    # --- LÆ¯U RAW FEATURES (JSON) ---
    print(f"\nÄang lÆ°u raw features vÃ o file: {output_json_path}")
    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:
        json.dump(raw_features, jsonfile, indent=2, default=str)
    print("ÄÃ£ lÆ°u raw features!")
    
    # --- LÆ¯U FEATURE VECTOR CHI TIáº¾T (CSV) ---
    print(f"\nÄang lÆ°u feature vector vÃ o file: {output_csv_path}")
    
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # Header cho CSV
        writer.writerow(['FeatureGroup', 'FeatureIndex', 'LocalIndex', 'FeatureName', 'Value'])
        
        current_pos = 0
        
        # Äá»‹nh nghÄ©a tÃªn cÃ¡c nhÃ³m Ä‘áº·c trÆ°ng theo thá»© tá»± trong EMBER 2018
        feature_groups = [
            ('ByteHistogram', 256),
            ('ByteEntropyHistogram', 256),
            ('StringExtractor', 104),
            ('GeneralFileInfo', 10),
            ('HeaderFileInfo', 62),
            ('SectionInfo', 255),
            ('ImportsInfo', 1280),
            ('ExportsInfo', 128)
        ]
        
        for group_name, expected_dim in feature_groups:
            # Láº¥y slice tÆ°Æ¡ng á»©ng
            feature_slice = feature_vector[current_pos : current_pos + expected_dim]
            
            # Ghi tá»«ng giÃ¡ trá»‹
            for local_idx, value in enumerate(feature_slice):
                global_idx = current_pos + local_idx
                
                # Táº¡o tÃªn Ä‘áº·c trÆ°ng cÃ³ Ã½ nghÄ©a
                feature_name = f"{group_name}_{local_idx}"
                
                # ThÃªm tÃªn cá»¥ thá»ƒ hÆ¡n cho má»™t sá»‘ nhÃ³m
                if group_name == 'ByteHistogram':
                    feature_name = f"Byte_{local_idx:02X}_Count"
                elif group_name == 'ByteEntropyHistogram':
                    feature_name = f"ByteEntropy_Bin_{local_idx}"
                elif group_name == 'GeneralFileInfo':
                    info_names = ['Size', 'VirtualSize', 'HasDebug', 'Exports', 
                                  'Imports', 'HasSignature', 'HasTLS', 'HasResources',
                                  'NumSections', 'Timestamp']
                    if local_idx < len(info_names):
                        feature_name = f"General_{info_names[local_idx]}"
                
                row = [group_name, global_idx, local_idx, feature_name, value]
                writer.writerow(row)
            
            current_pos += expected_dim
        
    print("ÄÃ£ lÆ°u feature vector thÃ nh cÃ´ng!")
    
    # --- THá»NG KÃŠ Tá»”NG QUAN ---
    print("\n--- THá»NG KÃŠ Äáº¶C TRÆ¯NG ---")
    current_pos = 0
    for group_name, dim in feature_groups:
        feature_slice = feature_vector[current_pos : current_pos + dim]
        non_zero = np.count_nonzero(feature_slice)
        print(f"{group_name:25s}: {dim:4d} chiá»u, {non_zero:4d} giÃ¡ trá»‹ khÃ¡c 0")
        current_pos += dim
    
    print(f"\nTá»•ng cá»™ng: {current_pos} Ä‘áº·c trÆ°ng")
    print(f"\nFile CSV: {output_csv_path}")
    print(f"File JSON: {output_json_path}")

except FileNotFoundError:
    print(f"Lá»–I: KhÃ´ng tÃ¬m tháº¥y file táº¡i '{file_to_analyze}'.")
except Exception as e:
    print(f"ÄÃ£ xáº£y ra lá»—i: {e}")
    import traceback
    traceback.print_exc()